{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([4,7])\n",
    "y = torch.Tensor([7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28., 63.])\n"
     ]
    }
   ],
   "source": [
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, \n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datasets.MNIST(\"\", train=False, download=True, \n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([6, 3, 3, 6, 6, 9, 7, 5, 6, 6])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0 }\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.1207e-01, 7.6068e-01, 4.7782e-01, 6.1594e-01, 2.2668e-01, 4.5014e-01,\n",
       "         8.0490e-01, 2.5433e-01, 8.0994e-01, 2.4771e-01, 7.7729e-01, 2.9467e-01,\n",
       "         7.7241e-01, 7.8522e-01, 5.0063e-01, 8.1695e-01, 1.1868e-01, 4.9612e-01,\n",
       "         8.5770e-02, 5.6577e-01, 9.0402e-01, 3.8164e-01, 2.2249e-02, 5.6523e-02,\n",
       "         8.4532e-01, 9.9932e-01, 5.1726e-01, 6.4782e-01, 3.2016e-01, 6.3804e-01,\n",
       "         6.7549e-01, 8.8154e-01, 5.6695e-01, 6.9265e-01, 5.7330e-01, 2.1952e-01,\n",
       "         7.1960e-01, 9.3033e-01, 2.3795e-01, 7.5444e-01, 2.7251e-01, 2.7408e-01,\n",
       "         2.2463e-01, 9.4722e-01, 3.6909e-02, 1.3158e-02, 6.7560e-01, 4.4194e-02,\n",
       "         2.8378e-01, 8.7030e-01, 9.5233e-01, 1.0632e-01, 9.0120e-01, 4.8691e-01,\n",
       "         4.1803e-01, 6.9999e-01, 7.1380e-01, 8.1327e-01, 1.8572e-01, 9.2253e-01,\n",
       "         8.0062e-01, 8.2855e-01, 9.0421e-01, 9.5525e-01, 7.3421e-01, 7.4181e-01,\n",
       "         9.4425e-01, 5.0361e-01, 5.5642e-02, 6.0547e-01, 5.5411e-01, 2.4411e-01,\n",
       "         4.1602e-02, 8.2875e-01, 4.2000e-02, 8.4560e-01, 3.7874e-01, 1.6819e-01,\n",
       "         2.1635e-02, 5.1972e-01, 2.0213e-01, 8.8118e-01, 3.9858e-01, 1.9660e-01,\n",
       "         9.6249e-02, 1.5634e-01, 1.5055e-01, 5.0302e-01, 8.9237e-01, 3.5846e-01,\n",
       "         1.9061e-01, 4.8934e-01, 6.1137e-02, 3.8252e-01, 6.5804e-01, 7.6515e-02,\n",
       "         8.5504e-02, 9.6622e-01, 6.7854e-03, 2.5316e-01, 6.7185e-01, 5.7140e-01,\n",
       "         2.1380e-01, 9.5484e-01, 8.5408e-02, 2.1020e-01, 8.9261e-02, 9.3366e-01,\n",
       "         8.4916e-01, 9.6390e-01, 1.9210e-01, 9.0339e-01, 1.7250e-01, 8.1868e-01,\n",
       "         1.7046e-01, 7.7936e-01, 5.9778e-01, 4.5765e-01, 7.8120e-01, 2.4034e-01,\n",
       "         6.4921e-01, 8.3444e-01, 7.6663e-01, 8.9169e-01, 5.3599e-01, 4.2278e-02,\n",
       "         1.5253e-01, 9.0500e-01, 6.7351e-01, 7.8081e-01, 6.0323e-01, 1.7993e-01,\n",
       "         6.2088e-01, 5.1876e-01, 3.8812e-01, 5.2076e-01, 7.3100e-01, 8.8260e-01,\n",
       "         4.2949e-01, 4.3893e-01, 5.8928e-01, 3.2613e-03, 2.6378e-01, 8.5447e-01,\n",
       "         5.6179e-01, 5.3828e-01, 3.9713e-01, 7.4302e-01, 6.1266e-02, 5.7544e-01,\n",
       "         1.3590e-02, 9.8837e-01, 1.2563e-01, 9.9991e-01, 4.2573e-01, 1.1531e-01,\n",
       "         6.3675e-01, 5.7437e-01, 2.7833e-01, 7.0729e-01, 8.0517e-01, 1.0829e-01,\n",
       "         6.6529e-01, 4.7777e-01, 4.7482e-01, 2.8150e-01, 5.1703e-01, 8.4644e-01,\n",
       "         7.0539e-01, 3.8313e-01, 8.8367e-01, 5.4617e-01, 9.2823e-01, 4.9042e-01,\n",
       "         9.5554e-01, 6.9567e-01, 6.5977e-01, 5.7291e-01, 9.1737e-01, 1.7965e-01,\n",
       "         8.0582e-01, 6.8567e-01, 4.5561e-01, 8.7246e-01, 3.6468e-01, 7.8922e-01,\n",
       "         4.8742e-01, 6.5091e-01, 7.3039e-01, 5.4000e-01, 3.3567e-01, 5.5864e-01,\n",
       "         1.5234e-01, 4.4466e-01, 9.3418e-01, 8.1616e-01, 4.4932e-01, 4.0087e-01,\n",
       "         8.9965e-02, 7.3887e-01, 7.2062e-01, 2.3739e-01, 3.4657e-01, 9.5500e-01,\n",
       "         9.1956e-01, 9.1052e-01, 6.9164e-01, 7.7666e-01, 6.0816e-01, 4.9514e-01,\n",
       "         3.6368e-01, 9.4198e-02, 6.5204e-01, 2.9540e-01, 1.5811e-01, 1.5957e-01,\n",
       "         9.5478e-01, 6.5303e-01, 6.2963e-01, 3.1039e-01, 5.8425e-01, 7.5722e-01,\n",
       "         5.0125e-01, 9.7647e-01, 6.4054e-01, 4.4534e-01, 6.9907e-02, 8.0055e-01,\n",
       "         6.4585e-01, 3.3678e-01, 7.7342e-01, 7.9288e-02, 1.8222e-01, 3.3553e-02,\n",
       "         1.5408e-01, 3.1203e-04, 6.3887e-01, 3.9944e-01, 7.3180e-01, 5.3688e-01,\n",
       "         3.6108e-01, 7.6300e-02, 3.5313e-01, 8.6208e-02, 1.3128e-01, 2.8535e-01,\n",
       "         8.7862e-01, 5.5072e-01, 1.1419e-02, 2.0682e-01, 8.3115e-01, 1.3151e-01,\n",
       "         8.1857e-01, 6.8429e-01, 8.5395e-01, 2.0620e-01, 5.9854e-01, 3.5186e-01,\n",
       "         3.2236e-01, 7.0708e-01, 7.5175e-01, 5.5399e-01, 6.7971e-01, 9.2364e-02,\n",
       "         3.4351e-01, 9.7546e-01, 6.2516e-01, 7.9970e-01, 6.3457e-03, 4.2450e-01,\n",
       "         3.5712e-01, 6.1066e-01, 4.2221e-02, 6.3448e-02, 5.4763e-01, 1.7725e-01,\n",
       "         4.9854e-01, 1.2773e-01, 9.4792e-01, 2.5366e-01, 5.5001e-01, 2.5305e-01,\n",
       "         6.4694e-02, 7.9191e-01, 2.5937e-01, 1.8302e-01, 1.9952e-01, 2.3566e-01,\n",
       "         4.9482e-02, 3.7635e-01, 2.1027e-01, 5.8933e-03, 8.0818e-01, 3.4213e-01,\n",
       "         6.1907e-01, 4.2447e-01, 7.9245e-01, 2.8228e-01, 1.2843e-02, 6.7055e-01,\n",
       "         1.1261e-01, 7.6880e-01, 7.1469e-01, 7.6248e-01, 8.4790e-01, 2.4631e-01,\n",
       "         7.1964e-01, 5.9172e-01, 6.5813e-01, 8.2396e-01, 4.1983e-01, 2.5662e-01,\n",
       "         8.7018e-01, 6.9081e-01, 2.9642e-03, 4.8143e-01, 8.0184e-01, 7.5987e-01,\n",
       "         9.3030e-01, 1.4384e-01, 8.5078e-01, 5.2192e-01, 3.5357e-02, 8.2680e-01,\n",
       "         3.1719e-01, 6.3507e-01, 8.0081e-01, 3.6131e-01, 6.6969e-01, 8.0935e-01,\n",
       "         9.1493e-01, 5.9133e-01, 8.1208e-01, 1.5646e-01, 3.5192e-01, 1.8839e-01,\n",
       "         2.5654e-01, 5.9820e-01, 2.8582e-01, 8.8564e-01, 3.9414e-01, 3.5467e-02,\n",
       "         5.6977e-01, 6.7945e-01, 9.3264e-01, 6.9658e-01, 8.3418e-02, 2.8698e-02,\n",
       "         9.6571e-01, 3.4863e-01, 7.4359e-01, 8.0401e-01, 2.2217e-01, 9.1317e-01,\n",
       "         1.9865e-01, 3.4590e-01, 6.0066e-01, 9.6631e-01, 1.3618e-01, 2.4178e-01,\n",
       "         4.7409e-01, 4.3056e-01, 6.2316e-01, 7.8593e-01, 6.3884e-01, 9.2844e-01,\n",
       "         1.2981e-01, 9.5825e-01, 3.3804e-02, 7.8547e-01, 8.9086e-01, 2.7918e-01,\n",
       "         5.7692e-01, 5.4200e-01, 2.3794e-01, 4.8353e-01, 3.7571e-01, 4.1813e-01,\n",
       "         6.8318e-01, 1.4315e-01, 4.5139e-02, 9.5615e-01, 5.7870e-01, 5.3724e-01,\n",
       "         3.5368e-01, 1.4775e-01, 1.0964e-01, 5.6813e-01, 3.6170e-01, 3.7708e-01,\n",
       "         8.3270e-01, 7.4071e-01, 4.2969e-01, 8.8410e-01, 9.0079e-01, 7.6334e-01,\n",
       "         3.1443e-01, 2.7660e-01, 1.8702e-03, 8.5519e-01, 1.3143e-01, 2.5061e-01,\n",
       "         5.6082e-01, 4.6999e-01, 1.1724e-01, 8.9931e-01, 3.5669e-01, 6.1522e-01,\n",
       "         7.9851e-01, 1.1120e-01, 8.4241e-01, 6.0697e-01, 5.0454e-01, 4.6022e-01,\n",
       "         8.2030e-01, 1.5239e-01, 5.9591e-01, 1.5628e-01, 1.3211e-01, 8.7703e-01,\n",
       "         1.3768e-02, 4.0629e-02, 4.5010e-01, 5.4663e-01, 2.0324e-01, 4.6946e-01,\n",
       "         6.4161e-01, 4.4170e-01, 3.7855e-01, 3.9333e-01, 8.1611e-01, 5.8589e-01,\n",
       "         7.3736e-01, 8.4584e-01, 4.9718e-01, 6.6866e-01, 1.7669e-01, 7.8254e-02,\n",
       "         6.4454e-01, 6.0419e-01, 4.8702e-01, 4.5773e-01, 7.9901e-01, 3.0757e-01,\n",
       "         9.8815e-01, 6.6848e-01, 8.6854e-01, 3.0484e-01, 4.8482e-01, 5.2087e-01,\n",
       "         8.9552e-01, 5.4001e-03, 8.7534e-03, 3.2152e-01, 9.4978e-01, 2.4635e-01,\n",
       "         6.7193e-01, 9.2040e-01, 7.8451e-01, 7.4292e-01, 1.6663e-01, 5.0258e-01,\n",
       "         4.9331e-01, 9.3927e-01, 5.1931e-02, 5.5894e-01, 4.9167e-01, 5.6367e-01,\n",
       "         4.8044e-01, 1.9004e-01, 2.3709e-01, 6.8989e-01, 4.9504e-01, 8.2240e-03,\n",
       "         5.5427e-02, 5.1153e-01, 8.5455e-01, 7.9466e-01, 1.8578e-01, 4.9448e-02,\n",
       "         2.2532e-01, 1.5070e-01, 9.3725e-01, 9.3871e-01, 2.0787e-01, 4.2596e-01,\n",
       "         2.0336e-01, 1.2200e-01, 4.1383e-01, 9.9921e-01, 8.2504e-01, 1.0925e-01,\n",
       "         4.2102e-01, 5.0454e-01, 6.2554e-01, 1.2356e-02, 6.4352e-01, 9.9319e-01,\n",
       "         1.1812e-01, 2.7323e-01, 7.1231e-01, 4.5057e-01, 5.7269e-01, 6.4518e-02,\n",
       "         5.9338e-02, 5.1645e-01, 7.7179e-01, 1.0116e-01, 9.6680e-01, 8.3456e-01,\n",
       "         2.5680e-01, 2.1342e-01, 9.8467e-01, 7.1233e-01, 1.2859e-01, 2.5279e-01,\n",
       "         1.0888e-01, 2.5960e-01, 4.4721e-01, 8.6262e-01, 7.6621e-01, 4.9273e-01,\n",
       "         9.4333e-01, 9.9845e-01, 3.8195e-01, 3.3782e-01, 3.9343e-01, 3.0865e-01,\n",
       "         2.8418e-01, 7.0799e-01, 4.1461e-03, 6.5754e-01, 7.9150e-01, 1.4923e-01,\n",
       "         2.7053e-01, 1.1160e-01, 3.5814e-01, 9.5117e-01, 1.1338e-01, 3.6077e-01,\n",
       "         5.8704e-01, 5.8039e-01, 7.9883e-01, 2.6383e-01, 5.0921e-01, 5.7214e-01,\n",
       "         8.8767e-01, 1.0429e-01, 4.7400e-01, 4.3176e-03, 5.8144e-01, 2.9106e-01,\n",
       "         1.2531e-01, 3.5789e-01, 1.9100e-01, 1.6655e-01, 7.2873e-01, 6.0890e-01,\n",
       "         3.8320e-02, 5.8037e-01, 7.0185e-01, 4.7215e-01, 5.4569e-01, 9.9429e-01,\n",
       "         5.0380e-01, 5.4980e-01, 8.8947e-01, 1.2962e-01, 8.8599e-01, 9.8732e-01,\n",
       "         8.1506e-01, 1.5668e-01, 8.0274e-01, 1.3794e-01, 4.0784e-01, 7.2531e-01,\n",
       "         5.5132e-01, 6.7445e-01, 3.4018e-01, 3.1523e-01, 4.1903e-01, 4.9314e-01,\n",
       "         8.9285e-01, 9.0565e-01, 9.5252e-01, 2.2988e-01, 2.7374e-01, 3.6227e-01,\n",
       "         6.9812e-02, 7.2484e-01, 1.3116e-02, 8.8237e-01, 6.3547e-01, 9.5133e-01,\n",
       "         9.4528e-01, 6.9274e-01, 5.1376e-01, 9.6059e-01, 9.6026e-01, 7.6879e-01,\n",
       "         1.2941e-01, 3.8626e-01, 3.2192e-01, 9.9014e-01, 1.9959e-01, 1.2092e-01,\n",
       "         7.5905e-01, 4.6698e-01, 2.0281e-01, 2.0978e-01, 7.2443e-01, 7.0313e-01,\n",
       "         7.0796e-01, 2.7649e-01, 7.5705e-01, 8.6235e-01, 5.5130e-01, 4.9315e-01,\n",
       "         2.5256e-01, 5.7819e-01, 7.2331e-01, 9.9328e-01, 1.1654e-01, 2.9791e-02,\n",
       "         8.5904e-01, 5.1294e-01, 5.0314e-01, 1.1316e-01, 7.5948e-01, 8.7240e-01,\n",
       "         7.5971e-01, 5.7226e-04, 5.3438e-01, 6.3452e-01, 6.7094e-01, 8.5970e-01,\n",
       "         8.6019e-01, 6.8090e-01, 8.8984e-01, 4.6527e-01, 5.1615e-01, 1.8800e-01,\n",
       "         3.5152e-01, 4.5796e-01, 1.6581e-01, 6.6691e-02, 7.6321e-01, 9.5910e-01,\n",
       "         7.5552e-02, 8.5519e-02, 6.3579e-01, 2.0627e-02, 7.5565e-01, 3.5510e-01,\n",
       "         6.8959e-01, 5.1050e-01, 2.6457e-02, 2.0475e-01, 8.9262e-02, 6.8365e-01,\n",
       "         7.0139e-01, 8.0334e-01, 2.4306e-02, 8.6696e-02, 6.7394e-01, 1.4183e-02,\n",
       "         9.8227e-01, 2.2077e-01, 4.0286e-01, 7.2568e-01, 7.8006e-01, 4.9522e-01,\n",
       "         3.0340e-02, 8.8586e-01, 8.3647e-01, 8.7251e-02, 8.2624e-01, 7.3356e-02,\n",
       "         8.6212e-01, 6.2487e-01, 7.7067e-02, 2.4606e-02, 5.0582e-02, 1.9169e-01,\n",
       "         5.8587e-01, 7.6040e-01, 7.7461e-01, 7.3389e-01, 9.8029e-01, 2.6952e-01,\n",
       "         6.9397e-02, 4.1304e-01, 5.8654e-01, 4.3390e-01, 1.0142e-01, 5.5053e-01,\n",
       "         9.7762e-01, 8.7098e-01, 4.6052e-01, 4.0740e-01, 7.3863e-01, 8.6008e-01,\n",
       "         4.9619e-01, 6.2673e-02, 8.6845e-01, 1.2890e-01, 6.9916e-01, 6.9974e-01,\n",
       "         4.2486e-01, 6.0056e-01, 9.4685e-01, 7.6201e-01, 4.9420e-01, 7.0451e-01,\n",
       "         3.5765e-01, 7.8386e-01, 2.8354e-01, 7.4577e-02, 5.4637e-01, 5.7900e-01,\n",
       "         1.9893e-01, 4.9818e-02, 2.2961e-01, 1.4057e-01, 4.6843e-01, 4.1085e-01,\n",
       "         9.0035e-01, 2.9994e-01, 4.0593e-01, 9.5487e-01, 3.5118e-01, 1.5620e-01,\n",
       "         1.7718e-01, 6.7498e-01, 3.7342e-01, 9.0727e-01, 3.6846e-01, 8.1366e-01,\n",
       "         5.2066e-01, 9.0660e-01, 1.7164e-01, 7.7952e-02, 1.2577e-01, 1.8006e-01,\n",
       "         2.5494e-01, 9.4774e-01, 2.4333e-01, 7.7808e-01, 9.8349e-01, 3.8651e-01,\n",
       "         6.3743e-01, 5.2457e-02, 5.2601e-01, 3.6537e-01, 1.5973e-01, 1.8133e-01,\n",
       "         6.6990e-01, 9.3359e-01, 9.4175e-02, 6.7736e-01, 9.5432e-01, 3.4787e-01,\n",
       "         1.2855e-01, 2.3022e-01, 8.5609e-01, 8.6482e-01, 7.6500e-01, 6.3256e-01,\n",
       "         5.9089e-01, 9.7340e-01, 4.5150e-02, 8.5839e-01, 7.7637e-01, 9.5832e-01,\n",
       "         1.3697e-01, 2.6573e-01, 5.6011e-01, 5.6579e-01, 4.0806e-01, 6.4083e-01,\n",
       "         1.9385e-01, 9.1784e-01, 3.6050e-01, 5.9394e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3995, -2.3015, -2.3681, -2.3272, -2.3561, -2.2239, -2.3084, -2.2227,\n",
       "         -2.2841, -2.2507]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6508, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2176, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output =  net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy\", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 9, 9, 9, 1, 9, 7, 1, 2, 7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOiUlEQVR4nO3dcYxV5ZnH8d/jiNhgFQYKHZWsgtjUlVTcWdS1W3VJWyXZALuxK2kauoFO0y2JTUxT1/4B3aSpqds2zcqyGQsRaxfTbTXSrrGlExq2uw1lpFQQuotLqYwQsBKLunZkmGf/mEMzxTnvGe859547PN9PMrn3nueeex6P/Obcue859zV3F4Bz33l1NwCgNQg7EARhB4Ig7EAQhB0I4vxWbuwCm+wXakorNwmE8ju9rjd90MaqlQq7md0u6euSOiR9w93vTz3/Qk3RDbaozCYBJOzwvtxaw2/jzaxD0jpJd0i6RtJyM7um0dcD0Fxl/mZfKOl5dz/o7m9KekzSkmraAlC1MmG/TNLhUY8HsmV/wMx6zKzfzPpPabDE5gCUUSbsY30I8JZzb92919273b17kiaX2ByAMsqEfUDS7FGPL5d0pFw7AJqlTNh3SppnZlea2QWS7pK0pZq2AFSt4aE3dx8ys9WSfqCRobeN7v5cZZ0BqFSpcXZ3f0rSUxX1AqCJOF0WCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EESpKZvN7JCkVyWdljTk7t1VNAWgeqXCnrnN3X9TwesAaCLexgNBlA27S/qhmT1jZj1jPcHMesys38z6T2mw5OYANKrs2/ib3f2Imc2UtNXMfunu20c/wd17JfVK0sXW6SW3B6BBpY7s7n4kuz0u6QlJC6toCkD1Gg67mU0xs3eeuS/pQ5L2VtUYgGqVeRs/S9ITZnbmdf7V3Z+upCtMGB1TL0nWB6+/Krf261Wnk+veMuf5ZL139vZkfVj5fzUuvXFJct2hwwPJ+kTUcNjd/aCk91XYC4AmYugNCIKwA0EQdiAIwg4EQdiBIKq4EAYT2PmzL0/WOx4dStY/OGN/st4z9Ue5tfMKjjXDGi6oN75+0X/X0C3J8oTEkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCc/RzQcc3VubVffmpact0Df7U+WU9dJipJ58mS9Z2D+ceTLb9dkFz3e4euTdZ/dyB9eW3fXQ/k1v75yu8k1131p59K1n3nnmS9HXFkB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGefAF5eeVOy/uDnH8ytLZhc7prwda/MTdY3rV+crHf1vZRbO73/QHLdS7UvWdfC+en6Xfml2zZ/NrnqnJ0/Tb/2BMSRHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJy9Dbz+9Jxk/afz88fRpfQ15R/Y8zfJdQf/bVayPn1Derx5pv4rWU9PylzO0EUXJOtdHe/IrV394OH0azfUUXsrPLKb2UYzO25me0ct6zSzrWZ2ILtNf0MCgNqN5238w5JuP2vZvZL63H2epL7sMYA2Vhh2d98u6cRZi5dI2pTd3yRpacV9AahYox/QzXL3o5KU3c7Me6KZ9ZhZv5n1n9Jgg5sDUFbTP413915373b37kma3OzNAcjRaNiPmVmXJGW3x6trCUAzNBr2LZJWZPdXSHqymnYANEvhOLuZbZZ0q6QZZjYgaY2k+yV928xWSnpB0p3NbHKiO/jl9PXo+wrG0YvmKV/4pbtza12P7M2tSdLpkweT9VoVXK++9J/y536X0vtt35p3J9e9etVAsj4RFYbd3ZfnlBZV3AuAJuJ0WSAIwg4EQdiBIAg7EARhB4LgEtcKDC36k2R930fTQ2vHTr+RrK/66/T0wTN35l9m2sxLTMcj9TXYF955LLnutvkPJ+tF00Wnvib76t54p25zZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnr4Kny0WXqC5bm54+uLPG6YOLpot+8y9fSda/f/0DubXUVz1Lxfut6FiVnG76Z3sKXvvcw5EdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0CL96SnunmvILfqZ17Xyu1/aG/yL+e/lfL0v+L71n0VLLec0nj00VL0rDyx9KL1i06Fv37/12SrG/98B8nqi8WbPvcw5EdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0CVvJ69oe+sz5Z/5eX/yxZ/8LM3oa3XXQOQNH67/1xT7K+6aYNubWFk9M7rmjbX/zSx5L1zoH6vgegHRUe2c1so5kdN7O9o5atNbMXzWx39rO4uW0CKGs8b+MflnT7GMu/5u7XZT/p07AA1K4w7O6+XdKJFvQCoInKfEC32syezd7mT8t7kpn1mFm/mfWfUrz5tYB20WjY10uaK+k6SUclfSXvie7e6+7d7t49SekLRgA0T0Nhd/dj7n7a3YclPSRpYbVtAahaQ2E3s65RD5dJ2pv3XADtwdzTY51mtlnSrZJmSDomaU32+DqNfGP6IUmfdPejRRu72Dr9BltUquF2dP7sy5P1jkeHkvUnrkoPZgwXfDH9muMLcmvfO3Rtct3J309fEz59Q3qsumNqev07/vNQbu3vpv4que57frwyWZ/70Z8n6xHt8D6d9BNjflFA4Uk17r58jMX5Z0oAaEucLgsEQdiBIAg7EARhB4Ig7EAQXOJagaHDA+n6Len1P3zbqlLb79i2K7d2qfaVeu0iJx+bnqz3TP1Rbm3nYPpYc8U3ir5qGm8HR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9jaQGiev28srb0rWd8xfl6wPJ44nq7+4Ornu9G18FXSVOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs0e3cH6y/PiaB5L1Yb0jWV/3ytzcWtHXVKNaHNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2c9xRdNJ//YfXk/WuzrS4+jHTr+RrD/98T9PVPck10W1Co/sZjbbzLaZ2X4ze87M7s6Wd5rZVjM7kN1Oa367ABo1nrfxQ5Lucff3SrpR0qfN7BpJ90rqc/d5kvqyxwDaVGHY3f2ou+/K7r8qab+kyyQtkbQpe9omSUub1SSA8t7WB3RmdoWkBZJ2SJrl7kelkV8IkmbmrNNjZv1m1n9Kg+W6BdCwcYfdzC6S9F1Jn3H3k+Ndz9173b3b3bsnaXIjPQKowLjCbmaTNBL0b7n749niY2bWldW7JB1vTosAqlA49GZmJmmDpP3u/tVRpS2SVki6P7t9sikdolBqeO19W15IrvuFmT9P1oc1nKwvW/vZZL1zJ5extovxjLPfLOljkvaY2e5s2X0aCfm3zWylpBck3dmcFgFUoTDs7v4TSZZTXlRtOwCahdNlgSAIOxAEYQeCIOxAEIQdCIJLXCeAostUF//gF7m1nksOJdctukT1ts3pcfQ5GxlHnyg4sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzTwAdjw4l66mx9KLr0e94pidZn/M5xtHPFRzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnbwMnlNybr269al6xPso7c2t++cGty3UuX7UvWce7gyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYxnfvbZkh6R9G5Jw5J63f3rZrZW0ickvZQ99T53f6pZjZ7L3piR/p37s8G8SXRHdOh0bu3w388rWHdXso5zx3hOqhmSdI+77zKzd0p6xsy2ZrWvufs/Nq89AFUZz/zsRyUdze6/amb7JV3W7MYAVOtt/c1uZldIWiBpR7ZotZk9a2YbzWxazjo9ZtZvZv2nNFiqWQCNG3fYzewiSd+V9Bl3PylpvaS5kq7TyJH/K2Ot5+697t7t7t2TNLmClgE0YlxhN7NJGgn6t9z9cUly92PuftrdhyU9JGlh89oEUFZh2M3MJG2QtN/dvzpqedeopy2TtLf69gBUxdw9/QSz90v6D0l7pN9/L/F9kpZr5C28Szok6ZPZh3m5LrZOv8EWlWwZQJ4d3qeTfmLMsdrxfBr/E0ljrcyYOjCBcAYdEARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiMLr2SvdmNlLkn49atEMSb9pWQNvT7v21q59SfTWqCp7+yN3f9dYhZaG/S0bN+t39+7aGkho197atS+J3hrVqt54Gw8EQdiBIOoOe2/N209p197atS+J3hrVkt5q/ZsdQOvUfWQH0CKEHQiilrCb2e1m9t9m9ryZ3VtHD3nM7JCZ7TGz3WbWX3MvG83suJntHbWs08y2mtmB7HbMOfZq6m2tmb2Y7bvdZra4pt5mm9k2M9tvZs+Z2d3Z8lr3XaKvluy3lv/NbmYdkv5H0gclDUjaKWm5u+9raSM5zOyQpG53r/0EDDP7gKTXJD3i7tdmy74s6YS735/9opzm7p9rk97WSnqt7mm8s9mKukZPMy5pqaSPq8Z9l+jrI2rBfqvjyL5Q0vPuftDd35T0mKQlNfTR9tx9u6QTZy1eImlTdn+TRv6xtFxOb23B3Y+6+67s/quSzkwzXuu+S/TVEnWE/TJJh0c9HlB7zffukn5oZs+YWU/dzYxh1plptrLbmTX3c7bCabxb6axpxttm3zUy/XlZdYR9rKmk2mn872Z3v17SHZI+nb1dxfiMaxrvVhljmvG20Oj052XVEfYBSbNHPb5c0pEa+hiTux/Jbo9LekLtNxX1sTMz6Ga3x2vu5/faaRrvsaYZVxvsuzqnP68j7DslzTOzK83sAkl3SdpSQx9vYWZTsg9OZGZTJH1I7TcV9RZJK7L7KyQ9WWMvf6BdpvHOm2ZcNe+72qc/d/eW/0harJFP5P9X0ufr6CGnrzmSfpH9PFd3b5I2a+Rt3SmNvCNaKWm6pD5JB7Lbzjbq7Zsamdr7WY0Eq6um3t6vkT8Nn5W0O/tZXPe+S/TVkv3G6bJAEJxBBwRB2IEgCDsQBGEHgiDsQBCEHQiCsANB/D+vnlgiPrNVWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[3].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[3].view(-1, 784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
